# FACTORIZED CONTRASTIVE LEARNING

> FACTORIZED CONTRASTIVE LEARNING:  Going Beyond Multi-view Redundancy
>
> WHAT TO ALIGN IN MULTIMODAL CONTRASTIVE  LEARNING?

这两篇文章中都使用了先对单模态做增强的方法，比较他们的异同。

## **第一部分: FactorCL 的捕获方法 (分解式)**

FactorCL 的核心思想是“**先分解，再优化**”。它为不同的信息成分设计了不同的、显式的优化目标。其理论基础是**条件互信息 (CMI)** 分解：

$I(X_1, X_2; Y) = \underbrace{I(X_1; X_2; Y)}_{S_{\text{cmi}}} + \underbrace{I(X_1; Y | X_2)}_{U_{1, \text{cmi}}} + \underbrace{I(X_2; Y | X_1)}_{U_{2, \text{cmi}}}$

### **1. 捕获任务相关的共享信息 (S)**

- **目标信息**: $S = I(X_1; X_2; Y) = I(X_1; X_2) - I(X_1; X_2|Y)$
  - 这个公式的直观含义是：共享信息 = 总共享信息 - 任务无关的共享信息。

- **捕获方法**: FactorCL 设计了一个目标函数，该函数**最大化**总共享信息的**下界**，同时**最小化**任务无关共享信息的**上界**。在自监督设置下，标签 `Y` 被多模态增强视图 `(X_1', X_2')` 替代。

    $S \ge \underbrace{I_{NCE}(X_1; X_2)}_{\text{最大化总共享}} - \underbrace{I_{NCE-CLUB}(X_1; X_2|X_1', X_2')}_{\text{最小化无关共享}}$

  - **`I_NCE(X_1; X_2)`**: 这是标准的对比学习损失 (如 InfoNCE)，通过将配对的 `(x_1, x_2)` 视为正样本，不配对的视为负样本，来拉近 `X1` 和 `X2` 的表示，从而捕获它们之间的全部共享信息。
  - **`I_NCE-CLUB(X_1; X_2|X_1', X_2')`**: 这是一个创新的项。它估计了在已知增强视图 `(X_1', X_2')`（即近似于已知任务信息）后，`X1` 和 `X2` 之间**仍然存在**的共享信息。这部分信息被认为是与任务无关的，因此通过最小化其上界来将其从表示中**移除**。

### **2. 捕获任务相关的独有信息 (Ui)**

- **目标信息**: $U_1 = I(X_1; Y | X_2) = I(X_1; Y) - I(X_1; X_2; Y) = I(X_1; Y) - (I(X_1; X_2) - I(X_1; X_2|Y))$
  - 直观含义是：`X1` 的独有信息 = `X1` 的总任务信息 - `X1` 和 `X2` 的共享任务信息。

- **捕获方法**: 类似地，FactorCL 通过最大化/最小化各项的下界/上界来构造目标函数。

    $U_i \ge \underbrace{I_{NCE}(X_i; X_i')}_{\text{捕获总任务信息}} - \underbrace{(I_{NCE-CLUB}(X_1; X_2) - I_{NCE}(X_1; X_2|X_1', X_2'))}_{\text{减去共享信息}}$

  - **`I_NCE(X_i; X_i')`**: 这一项通过对比一个模态 `Xi` 和其自身的增强版本 `Xi'` 来学习。根据“最优增强”假设，`I(X_i; X_i') ≈ I(X_i; Y)`，因此这一项捕获了 `Xi` 中包含的**全部**任务相关信息（包括独有的和共享的）。
  - **`-(...)`**：后面减去的项是上面计算出的共享信息的近似值。通过从总任务信息中减去共享信息，剩下的就是独有信息。

### **原文证明**

FactorCL 在其附录 C.3 中的 **Theorem 6** 对上述目标函数的合理性进行了形式化证明。

> **Theorem 6. (Contrastive estimators for shared and unique information)**. Under assumptions on single-view augmentations $I(X_1; Y ) = I(X_1, X_1')$ (Definition 8) and optimal multi-view augmentation $X_2'$ such that $I(X_1, X_2; X_1', X_2') = I(X_1, X_2; Y )$ (Definition 9), we can define contrastive objectives for task-relevant shared and unique information with:
>
> $S = I(X_1; X_2; Y ) \ge I_{NCE}(X_1; X_2) - I_{NCE-CLUB}(X_1; X_2|X_1', X_2')$
>
> $U_i = I(X_i; Y |X_{-i}) \ge I_{NCE}(X_i; X_i') - I_{NCE-CLUB}(X_1; X_2) + I_{NCE}(X_1; X_2|X_1', X_2')$
>
> **Proof.** The objectives follow from the fact that $I_{NCE}(X_1; X_2)$ and $I_{NCE}(X_1; X_2|X_1', X_2')$ are lower bounds of $I(X_1; X_2)$ and $I(X_1; X_2|Y)$ respectively, and $I_{NCE-CLUB}(X_1; X_2)$ and $I_{NCE-CLUB}(X_1; X_2|X_1', X_2')$ are upper bounds of $I(X_1; X_2)$ and $I(X_1; X_2|Y)$ respectively:
>
> $S = I(X_1; X_2; Y ) = I(X_1; X_2) - I(X_1; X_2|Y)$
> $\ge I_{NCE}(X_1; X_2) - I_{NCE-CLUB}(X_1; X_2|X_1', X_2')$
>
> $U_i = I(X_i; Y |X_{-i}) = I(X_i; Y ) - (I(X_1; X_2) - I(X_1; X_2|Y))$
> $\ge I_{NCE}(X_i; X_i') - (I_{NCE-CLUB}(X_1; X_2) - I_{NCE}(X_1; X_2|X_1', X_2'))$
>
> and symmetrically for U2.

---

### **第二部分: CoMM 的捕获方法 (融合式)**

CoMM 的核心思想是“**先融合，再对比**”。它不为每个信息成分设计单独的目标，而是设计了两个目标，并声称 R, U, S 会从这些目标中**自然涌现**。其理论基础是**部分信息分解 (PID)**：

$I(X_1, X_2; Y) = \underbrace{R_{\text{pid}}}_{\text{冗余}} + \underbrace{U_{1, \text{pid}}}_{\text{独有}} + \underbrace{U_{2, \text{pid}}}_{\text{独有}} + \underbrace{S_{\text{pid}}}_{\text{协同}}$

### **1. 捕获总信息 (R + U1 + U2 + S)**

- **目标信息**: 捕获与任务相关的全部信息 $I(X, Y) = R+U_1+U_2+S$。
- **捕获方法**: CoMM 的主要学习目标是最大化两个不同增强版本 `X'` 和 `X''` 经过**融合模块**后得到的**统一多模态表示** `Z'` 和 `Z''` 之间的互信息。

    $\mathcal{L} = - \hat{I}_{NCE}(Z', Z'')$

- **核心机制**: 论文假设存在“最小标签保持多模态增强” (Assumption 1)，使得 $I(X, X') = I(X, Y)$。通过最大化 `I(Z', Z'')`，模型被驱动去捕获 `I(X, X')` 中的所有信息，也就间接捕获了 `I(X, Y)` 中的所有信息，即 R, U1, U2, S 的总和。

### **2. 捕获冗余和独有信息 (R + Ui)**

- **目标信息**: 捕获单模态 `Xi` 所能提供的全部任务信息，即 $I(X_i; Y) = R + U_i$。
- **捕获方法**: CoMM 引入了一个辅助的对比目标。它通过**掩码 (masking)** 的方式，只让模态 `i` 的信息通过模型，得到一个近似的单模态表示 `Zi`。然后最大化这个 `Zi` 与之前那个完整的、增强的多模态表示 `Z'` (或 `Z''`) 之间的互信息。

    $\mathcal{L}_i = - \frac{1}{2} (\hat{I}_{NCE}(Z_i, Z') + \hat{I}_{NCE}(Z_i, Z''))$

- **核心机制**: 这个目标强制模型学习到的表示 `Z` 具有这样的特性：即使只给出其中一个模态的信息（通过 `Zi`），也应该能与完整的表示 `Z'` 对齐。这会驱动模型保留该单模态 `Xi` 中包含的所有任务信息。根据 PID 的一致性方程，这部分信息正是 `R + Ui`。

### **3. 捕获协同信息 (S)**

- **目标信息**: 协同信息 `S`。
- **捕获方法**: CoMM **没有为 `S` 设计直接的优化目标**。它认为 `S` 是**涌现**出来的。
- **核心机制**: 协同信息的定义是“必须同时看到 `X1` 和 `X2` 才能获得的信息”。在 CoMM 的两个损失项中，只有主要目标 $\mathcal{L} = - \hat{I}_{NCE}(Z', Z'')$ 需要模型**同时处理所有模态**来计算 `Z'` 和 `Z''`。而辅助目标 $\mathcal{L}_i$ 实际上只关注单模态信息。因此，模型为了优化主要目标 $\mathcal{L}$，**必须**学习如何利用多模态融合带来的 synergistic 信息。在 ablation study (Fig. 5) 中，论文也展示了单独优化 $\mathcal{L}_i$ 无法学习协同信息，而单独优化 $\mathcal{L}$ 虽然可以，但速度很慢，两者结合效果最好。

#### **原文证明**

CoMM 的理论 justification 来自其 **Lemma 2 和 Lemma 3** (附录 G)。

> **Proof 2 (Lemma 2)** Given data processing inequalities for the Markov chains $X \rightarrow X' \rightarrow Z'_{\theta}$ and $Z'_{\theta} \rightarrow X \rightarrow Z_{\theta}$, we have:
>
> $I(Z_{\theta};Z'_{\theta}) \le I(X, Z'_{\theta}) \le I(X, X')$
>
> The equality can be achieved, for example, by selecting $f_{\theta}(·) = Id(·)$, the identity function.

这个证明表明，最大化 $I(Z;Z')$ 的上限是 $I(X, X')$。结合 Assumption 1 ($I(X, X')=I(X,Y)$)，这就为**捕获总信息 (R+U+S)** 提供了理论依据。

> **Proof 3 (Lemma 3)** First, we prove that $I(Z'_{\theta \star} ; Y ) = I(X', Y )$.
> Indeed, we have:
> $I(X'; Y ) = I(X'; Y ; X) + I(X'; Y |X) = I(Z'_{\theta \star} ; Y ; X) \text{ (by lemma 1 in (Wang et al., 2022a))}$
> $= I(Z'_{\theta \star} ; Y ) - I(Z'_{\theta \star} ; Y |X) = I(Z'_{\theta \star} ; Y ) \text{ because } Z'_{\theta \star} = f_{\theta \star} (t(X))$
>
> Second, let $T = \{t_i\}$ such that $X' = t_i(X) = X_i$ and $Z'_{\theta \star} = f_{\theta \star} (X_i) = Z_i$ for $i \in \{1, 2\}$ (with a slight abuse of notation). Thanks to the previous result and by the consistency equations for $I(X_i; Y )$ in Eq. (2) (main paper), the final result follows:
>
> $I(Z_i; Y ) = I(Z'_{\theta \star} ; Y ) = I(X'; Y ) = I(X_i; Y ) = R + U_i$

这个证明是**捕获 R+Ui** 的关键。它表明，如果我们将增强函数 `t` 特殊化为仅选择第 `i` 个模态的“投影”操作，那么优化得到的表示 `Zi` 将保留 `Xi` 中关于 `Y` 的所有信息，根据 PID 理论，这部分信息就是 `R+Ui`。

## 理论目标与实践挑战

**两篇文章并非使用了完全相同的增强方法，而是遵循了同一个理论原则，但在实际应用哲学上有所不同。** 通过数据增强 `X'` 来近似一个“保留任务相关信息，改变任务无关信息”的理想视图，从而使得 $I(X, X') \approx I(X, Y)$。

这个等式是一个**理论目标**，在实践中无法被精确实现或验证，因为在自监督学习中我们并不知道 `Y`，也不知道究竟哪些信息是“任务相关”的。因此，研究人员通过一系列**启发式 (heuristic)** 的方法来**近似**这个目标。

下面，我将详细列出这些启发式方法是如何在实际中实现的，并对比两篇论文在应用这些方法时的不同侧重点。

---

### **一、 如何在实践中实现“标签保持”增强**

实现的核心思想是：**一个好的增强应该在低级特征（如位置、颜色、音调、措辞）上产生变化，但在高级语义（如物体类别、核心情感、文本含义）上保持不变。** 因为下游任务通常与高级语义相关。

以下是针对不同模态的主流实现方法：

#### **1. 图像 (Image)**

这是研究最成熟的领域，通常使用“SimCLR-style”的一系列增强组合：
- **RandomResizedCrop (随机裁剪并缩放)**：这是最关键的增强之一。它改变了物体的位置、大小和可见部分，但通常会保留核心物体本身。这迫使模型学习物体的内在特征，而不是其在图像中的特定位置。
- **ColorJitter (色彩抖动)**：随机改变亮度、对比度、饱和度和色调。这使得模型对颜色不敏感，更关注形状和纹理。
- **RandomGrayscale (随机灰度化)**：以一定概率将图像变为灰度图，进一步降低对颜色的依赖。
- **RandomHorizontalFlip (随机水平翻转)**：对于大多数物体（如猫、汽车），翻转不会改变其类别。
- **GaussianBlur (高斯模糊)**：轻微的模糊可以去除一些高频噪声，让模型关注更宏观的结构。

#### **2. 文本 (Text)**

文本增强比图像更具挑战性，因为微小的改动就可能改变语义。
- **Masking (掩码)**：随机遮盖一部分词元 (token)，如 BERT 的做法。模型需要根据上下文恢复信息，这能学习到丰富的语境表示。
- **Token Deletion/Shuffling (词元删除/打乱)**：小幅度地删除或打乱词的顺序，如果幅度不大，通常能保留句子主干含义。
- **Back-translation (回译)**：将句子翻译成另一种语言，再翻译回来。例如，"The car is fast" -> (法语) "La voiture est rapide" -> (英语) "The vehicle is quick"。措辞（低级特征）变了，但核心语义（高级特征）得以保留。这是一种效果很好但计算成本较高的方法。

#### **3. 音频/时间序列 (Audio/Time-Series)**

- **Adding Noise (添加噪声)**：在波形或频谱图上添加高斯噪声。
- **Time Masking (时间掩码)**：在频谱图上遮盖掉一小段时间。
- **Frequency Masking (频率掩码)**：在频谱图上遮盖掉一小段频率范围。（这两种掩码是 SpecAugment 的核心）
- **Pitch Shifting (音调偏移)**：改变音频的音高，但不改变内容（如说话的词语或音乐的旋律）。
- **Time Stretching (时间拉伸)**：在不改变音高的情况下，加快或减慢音频播放速度。

---

### **二、 FactorCL 和 CoMM 在应用这些方法时的哲学差异**

尽管它们可能使用上述工具箱中的相同工具（如都对图像使用随机裁剪），但它们**选择和组合这些工具的意图和方式**是不同的。

#### **CoMM 的方法：强力、通用、无条件 (Strong, General, Unconditional)**

CoMM 的哲学非常直接：它需要为其**统一的多模态表示 `Z`** 创造出两个尽可能不同的、但语义一致的“视图” `Z'` 和 `Z''`，以构造一个困难的对比任务。

- **实现方式**：对于每个模态，**独立地**应用一套**强力**的、通用的增强。例如，对图像 `X1` 应用一套完整的 SimCLR 增强得到 `X1'`，同时独立地对文本 `X2` 应用 Masking 得到 `X2'`，组合成 `X' = (X1', X2')`。
- **核心思想**：CoMM 认为，只要增强足够强，能够迫使模型在融合表示时忽略掉各模态内的低级噪声，就自然能学会关注跨模态的高级语义，包括冗余、独有和协同信息。它**不要求增强之间有任何依赖关系**。
- **原文佐证**：在 CoMM 的 Ablation Study (Table 5) 中，作者明确指出，对两个模态都应用强力增强（{All}）时效果最好，并且特意反驳了 FactorCL 的观点，强调 CoMM **不需要任务相关的、有条件的增强**，这突显了其框架的通用性。

#### **FactorCL 的方法：精细、小心、有条件 (Careful, Conditional)**

FactorCL 的需求更为复杂。它不仅仅是为了创造对比视图，更是为了用增强 `(X1', X2')` 来**近似任务标签 `Y`**，以便能有意义地估计**条件互信息** $I(X_1; X_2 | Y)$。这就对其增强策略提出了更高的要求。

- **实现方式**：FactorCL 提出了“**独有信息增强 (Unique Augmentation)**”的概念。这意味着对一个模态（如 `X2`）的增强，**需要考虑另一个模态（`X1`）的内容**，目的是**避免破坏它们之间的共享信息**。
- **核心思想**：如果一个标准的增强（如随机裁剪）破坏了 `X1` 和 `X2` 之间的共享信息，那么增强后的视图 `(X1', X2')` 就不能很好地近似 `Y`，从而导致对任务无关共享信息 $I(X_1; X_2 | Y)$ 的估计出现偏差。
- **一个具体的例子（来自原文）**：
  - **输入**: 文本 `X1` = "这辆车很快"，图片 `X2` = 一辆在高速公路上飞驰的汽车。
  - **共享信息**: “快”的概念，在图片中由“高速公路”这个背景元素体现。
  - **CoMM 式的增强**: 可能会对图片 `X2` 进行随机裁剪，恰好把“高速公路”裁掉了。对于 CoMM 来说问题不大，模型可以从其他特征学习。
  - **FactorCL 理想的增强**: 应该**避免**使用随机裁剪，而是使用**不影响背景**的 ColorJitter 或 HorizontalFlip。因为随机裁剪破坏了 `X1` 和 `X2` 都指向的“快”这个共享概念，使得 `(X1', X2')` 不再是 `Y` 的一个好代理。

### **总结**

| 对比维度 | **CoMM** | **FactorCL** |
| :--- | :--- | :--- |
| **增强目标** | 为**统一的多模态表示**创造强对比视图。 | 用增强视图**近似任务标签Y**，以计算条件互信息。 |
| **应用哲学** | **无条件的、独立的、强力的**。认为“越强越好”。 | **有条件的、依赖的、小心的**。需要避免破坏模态间的共享信息。 |
| **实现难度** | **更低**。可以直接套用各模态成熟的增强库。 | **更高**。需要根据具体的数据和任务，思考哪些增强是“安全”的，可能需要人工设计。 |
| **通用性** | **更强**。其方法不依赖于对模态间关系的先验知识。 | **可能更弱**。其“独有信息增强”策略可能需要为不同任务定制。 |

总而言之，两篇论文都将 $I(X, X') = I(X, Y)$ 作为理论基石，但在实践中，CoMM 采用了一种更简单、更鲁棒的近似方法，而 FactorCL 为了其更精细的分解目标，提出了一种更具挑战性、需要小心设计的条件增强策略。这也是 CoMM 在论文中批评 FactorCL 的假设“不切实际 (unrealistic)”的原因所在。

## 实际数据上应用时的增强方法

尽管它们的理论目标有细微差别，但在实践中，**CoMM 的介绍更为详尽和系统，而 FactorCL 的实现则更像是一个针对其理论的启发式简化**。

---

### **FactorCL (第一篇文章) 的增强方法**

FactorCL 的核心理论是“**最优多模态增强**”，特别是“**独有信息增强 (Unique Augmentation)**”，即在增强一个模态时，需要有条件地避免破坏与另一模态的共享信息。

作者在论文中主要通过 **IRFL (图像与比喻性语言)** 数据集来具体阐述他们是如何实践这一点的。

#### **1. 文本模态 (Text, `X1`)**

- **方法**: 作者提到对文本使用了**词语掩码 (Word Masking)**。这是一种非常标准的文本增强方法，与BERT等模型的预训练方式类似。
- **实现细节**: 在论文中，这部分没有进一步的详细参数描述，但通常这意味着随机选择一定比例的词元并用特殊标记 `[MASK]` 替换它们。

#### **2. 图像模态 (Image, `X2`) - 实践与理论的关键差异**

这是 FactorCL 增强方法的核心展示。作者对比了两种策略：

- **策略一：独立的、标准的增强 (FACTORCL-IndAug)**
  - **方法**: 对图像使用了一套标准的增强组合，包括**随机裁剪 (Cropping)**、**翻转 (Flipping)** 和 **色彩抖动 (Color Jittering)**。
  - **意图**: 这代表了不考虑“独有信息增强”理论的基线方法。

- **策略二：“独有信息增强”的实践 (FACTORCL-SSL)**
  - **方法**: 作者在附录D.3中明确指出：“**独有信息增强简单地移除了裁剪操作 (The unique augmentation simply removes the cropping operation)**”。这意味着增强只包括**翻转 (Flipping)** 和 **色彩抖动 (Color Jittering)**。
  - **意图与简化**: 这是对复杂理论的一个非常重要的**实践简化**。理论上，模型需要智能地识别并保留共享信息（比如不裁剪掉文本中提到的物体）。实践中，作者采用了一个简单的启发式规则：**在所有增强中，随机裁剪最有可能意外地移除一个完整的、具有语义信息的物体或场景元素**。因此，最直接、最“安全”地近似“不破坏共享信息”这一目标的方法，就是完全禁用裁剪。

**总结 (FactorCL)**:
FactorCL 在实践中并没有实现一个复杂的、能够感知另一模态内容的条件增强系统。而是通过一个简单的**操作禁用 (disabling an operation)** —— 即移除随机裁剪 —— 来近似其理论目标。这是一种务实但简化的解决方案。对于其他数据集（如视频、医疗数据），论文没有详细说明其“独有信息增强”的具体实现。

---

### **CoMM (第二篇文章) 的增强方法**

CoMM 的哲学是使用**强力的、通用的、无条件的**增强来为统一的多模态表示创造对比视图。作者在其附录B.2中，对不同数据类型使用的增强方法给出了一个**非常全面和详细的清单**。

#### **1. 图像 (Images)**

- **数据集**: Trifeature, MM-IMDb, Vision&Touch
- **方法**: 明确说明使用了“**默认的 SimCLR 增强** (default SimCLR augmentations)”。
- **实现细节**:
  - RandomResizedCrop (随机裁剪并缩放)
  - ColorJitter (色彩抖动)
  - RandomGrayscale (随机灰度化)
  - GaussianBlur (高斯模糊)
  - RandomHorizontalFlip (随机水平翻转)

#### **2. 表格数据 (Tabular Data)**

- **数据集**: MIMIC, Vision&Touch
- **方法**: 对每个特征分量**添加随机高斯噪声** (add a random Gaussian noise to each component)。

#### **3. 时间序列 (Time-Series)**

- **数据集**: MOSI, UR-FUNNY, MUsTARD, MIMIC, Vision&Touch
- **方法**: 作者特别指出，他们对时间序列的增强策略进行了基准测试（见附录C.1），最终选择了效果最好的组合。
- **实现细节**: **高斯噪声 (Gaussian noise)** 和 **随机丢弃 (Random Dropping)** 的随机组合，丢弃比例在序列的 0% 到 80% 之间。

#### **4. 原始文本 (Raw Text)**

- **数据集**: MM-IMDb
- **方法**: **随机掩码** (randomly mask)，类似于 BERT。
- **实现细节**: 随机掩码 **15%** 的输入词元。

**总结 (CoMM)**:
CoMM 的增强方法是其“简单通用”哲学的高度体现。它没有引入新的、复杂的增强理论，而是系统性地为每种数据类型选择了在单模态自监督学习中被证明**最有效、最强大的标准增强方法**。其方法描述清晰、可复现性强，并且与它的核心理论（强对比能涌现复杂交互）完全一致。

---

### **关键对比与总结**

| 特性 | FactorCL (实践中) | CoMM (实践中) |
| :--- | :--- | :--- |
| **整体哲学** | 将复杂理论**简化**为一个启发式规则。 | **系统性地应用**强大的标准方法。 |
| **图像增强** | 为了近似“独有信息增强”，**移除了随机裁剪**，只保留翻转和色彩抖动。 | 使用**全套强大的SimCLR增强**，包括随机裁剪。 |
| **文本增强** | 使用标准的词语掩码。 | 使用标准的词语掩码 (15%)。 |
| **其他模态** | 描述较少。 | 对表格、时间序列等多种模态给出了明确、具体的增强方法。 |
| **条件性** | **实践了“有条件”的思想**（通过移除一个“危险”操作）。 | **完全无条件**，对每个模态独立应用最强增强。 |

**最终结论**: 两篇论文都成功地将理论落地，但方式截然不同。FactorCL 的实践是其理论的一个巧妙但简化的代理，显示了理论与实践之间的差距。而 CoMM 的实践则是其理论的直接、有力支撑，展示了如何通过组合现有强大工具来实现一个概念上新颖的目标。CoMM 的方法在多样化数据集上的适用性和明确性上表现得更为出色。

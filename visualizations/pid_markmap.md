# PID方法综述

## 引言

- 部分信息分解是为了解决交互信息计算中可能出现负值的问题
  
  <!-- 信息论中互信息$I(X;Y)$量化了两个变量之间的统计依赖关系，
  然而当考虑两个或多个源变量与一个目标变量之间的信息的关系时，
  $I(X_1,X_2;Y)$无法区分这些信息是如何构成的。 -->
  <!-- 部分信息分解公理化地定义了各个部分信息：
  $$I(X_1,X_2;Y) = Red + Syn + Unq(X_1) + Unq(X_2)$$ -->
  <!-- 部分信息分解框架事实上没有直接定义所有四个信息原子，只为冗余信息$I_{red}$的概念建立了一套公理 -->

- 部分信息分解理论规定了冗余度量公理
  - 一个有效的冗余度量需要满足：
    - **对称性**：$I_{red}(X_1,X_2;Y) = I_{red}(X_2,X_1;Y)$
    - **自冗余**：$I_{red}(X_1;Y) = I(X_1;Y)$ 
      信源与自身关于目标的冗余就是全部互信息
    - **单调性**：$I_{red}(X_1,X_2;Y) \leq I_{red}(X_1;Y)$
- 计算得到冗余信息后可以进一步推得其它部分信息

## 冗余与协同信息的计算

- 基于特定信息的$I_{min}$
- 基于优化和博弈论的方法$I_{BORJA}$
- 基于逐点互信息的方法$I_{ccs},I_{pm}$
- 基于共同随机性的方法$I_{GK}$
- 基于"do-算子"的显式公式

## 深化理论与公理化基础

<!-- 这一方向的研究不直接提出新的度量，而是探索部分信息分解框架本身的数学结构、公理体系与其他理论框架的联系。 -->

- 公理体系的检验与扩展
- 格理论与逻辑基础
- 与其他理论框架的融合

## 计算各部分信息表示的方法

- 核心驱动力
  - 对传统多模态学习只最大化冗余信息的批判
  - 将PID信息原子转化为低维表示向量，将各部分信息编码为可学习的表示

- 具体方法
  - 在模型输出的表示层进行分离
  - 不进行显式分解，通过优化隐式包含
  - 通过专门化的子网络实现分解
  - 在最底层的单个神经元学习目标中实现分解

## 利用部分信息量的分析方法

- 驱动力
  - 对单一评分的批判，特征重要性分数无法描述复杂交互
  - 将特征或一个任务的整体贡献，分解为PID的信息原子以获得深入洞察
- 具体方法
  - DIP/PIDF
  - 输入：数据集和一训练的模型
  - 输出：每个特征的贡献

## 下游任务与应用

- 分析神经活动/脑区之间的协同关系
- 用信息量指导模型的训练
